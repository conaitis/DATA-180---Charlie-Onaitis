m2 <- as.matrix(tdm2)
sortterms <- sort(colSums(m2), decreasing = TRUE)
head(sortterms, 20)
news_dtm <- dfm(, stem = TRUE)
library("quanteda.textplots")
library("topicmodels")
library('tidytext')
install.packages("corpus")
# Custom options for knitting
knitr::opts_chunk$set(
message = FALSE,
warning = FALSE,
error = FALSE,
fig.align = "center",
cache = FALSE
)
library(tidyverse)
library(tm)
news<-read.csv("news.csv",header=T)
posWords <- scan("positive-words.txt", character(0), sep = "\n")  # 2006 items
negWords <- scan("negative-words.txt", character(0), sep = "\n")  # 4783 items
head(posWords,15)
head(negWords,15)
charVector <- news$headline_text
head(charVector, 6)
wordVector <- VectorSource(charVector)
wordCorpus <- Corpus(wordVector)
wordCorpus <- tm_map(wordCorpus, tolower)
wordCorpus <- tm_map(wordCorpus, removePunctuation)
wordCorpus <- tm_map(wordCorpus, removeNumbers)
wordCorpus <- tm_map(wordCorpus, removeWords, stopwords("english"))
print(news)
#my version of R does not allow me to download corpus packages, therefore I cannot use term_stats() function
wordCorpus2 <- Corpus(VectorSource(news$headline_text))
wordCorpus2 <- tm_map(wordCorpus, tolower)
wordCorpus2 <- tm_map(wordCorpus, removePunctuation)
wordCorpus2 <- tm_map(wordCorpus, removeNumbers)
wordCorpus2 <- tm_map(wordCorpus, removeWords, stopwords("english"))
tdm2 <- DocumentTermMatrix(wordCorpus2)
m2 <- as.matrix(tdm2)
sortterms <- sort(colSums(m2), decreasing = TRUE)
head(sortterms, 20)
library(tidyverse)
library(tm)
news<-read.csv("news.csv",header=T)
posWords <- scan("positive-words.txt", character(0), sep = "\n")  # 2006 items
negWords <- scan("negative-words.txt", character(0), sep = "\n")  # 4783 items
head(posWords,15)
head(negWords,15)
print(news)
#There is 18 years of data loaded in (2003-2021)
charVector <- news$headline_text
head(charVector, 6)
wordVector <- VectorSource(charVector)
wordCorpus <- Corpus(wordVector)
wordCorpus <- tm_map(wordCorpus, tolower)
wordCorpus <- tm_map(wordCorpus, removePunctuation)
wordCorpus <- tm_map(wordCorpus, removeNumbers)
wordCorpus <- tm_map(wordCorpus, removeWords, stopwords("english"))
print(news)
#A term document matrix depicts the frequency of terms that occur in a document or multiple documents.
tdm <- TermDocumentMatrix(wordCorpus)
m <- as.matrix(tdm)
wordCounts <- rowSums(m)
sort(wordCounts, decreasing = TRUE)
head(wordCounts, 10)
m <- as.matrix(tdm)
wordCounts <- rowSums(m)
sort(wordCounts, decreasing = TRUE)
head(wordCounts, 10)
m <- as.matrix(tdm)
wordCounts <- rowSums(m)
wordCounts <- sort(wordCounts, decreasing = TRUE)
head(wordCounts, 10)
many_word_counts <- wordCounts[wordCounts >= 50]
barplot(many_word_counts, las = 2, cex.names = 0.75, main = "Words That Showed Up 50 Times", xlab = "Words", ylab = "Frequency")
sum()
sum(wordCounts[posWords], na.rm = TRUE) /
barplot(many_word_counts[posWords >= 20], las = 2, cex.names = 0.75, main = "Positive Words That Showed Up 20 Times", xlab = "Words", ylab = "Frequency")
wordCorpus <- tm_map(wordCorpus, tolower)
wordCorpus <- tm_map(wordCorpus, removePunctuation)
wordCorpus <- tm_map(wordCorpus, removeNumbers)
wordCorpus <- tm_map(wordCorpus, removeWords, stopwords("english"))
print(news)
#A term document matrix depicts the frequency of terms that occur in a document or multiple documents.
tdm <- TermDocumentMatrix(wordCorpus)
m <- as.matrix(tdm)
wordCounts <- rowSums(m)
wordCounts <- sort(wordCounts, decreasing = TRUE)
head(wordCounts, 10)
wordCorpus <- tm_map(wordCorpus, tolower)
wordCorpus <- tm_map(wordCorpus, removePunctuation)
wordCorpus <- tm_map(wordCorpus, removeNumbers)
wordCorpus <- tm_map(wordCorpus, removeWords, stopwords("english"))
print(news)
wordCorpus <- tm_map(wordCorpus, tolower)
wordCorpus <- tm_map(wordCorpus, removePunctuation)
wordCorpus <- tm_map(wordCorpus, removeNumbers)
wordCorpus <- tm_map(wordCorpus, removeWords, stopwords("english"))
print(wordCorpus)
wordCorpus <- tm_map(wordCorpus, tolower)
wordCorpus <- tm_map(wordCorpus, removePunctuation)
wordCorpus <- tm_map(wordCorpus, removeNumbers)
wordCorpus <- tm_map(wordCorpus, removeWords, stopwords("english"))
wordCorpus.content[1]
wordCorpus <- tm_map(wordCorpus, tolower)
wordCorpus <- tm_map(wordCorpus, removePunctuation)
wordCorpus <- tm_map(wordCorpus, removeNumbers)
wordCorpus <- tm_map(wordCorpus, removeWords, stopwords("english"))
wordCorpus$content
wordCorpus <- tm_map(wordCorpus, tolower)
wordCorpus <- tm_map(wordCorpus, removePunctuation)
wordCorpus <- tm_map(wordCorpus, removeNumbers)
wordCorpus <- tm_map(wordCorpus, removeWords, stopwords("english"))
wordCorpus$content[1]
m <- as.matrix(tdm)
wordCounts <- rowSums(m)
wordCounts <- sort(wordCounts, decreasing = TRUE)
head(wordCounts, 10)
sum(wordCounts[posWords], na.rm = TRUE) /
barplot(many_word_counts[posWords >= 20], las = 2, cex.names = 0.75, main = "Positive Words That Showed Up 20 Times", xlab = "Words", ylab = "Frequency")
many_word_counts <- wordCounts[wordCounts >= 50]
barplot(many_word_counts, las = 2, cex.names = 0.75, main = "Words That Showed Up 50 Times", xlab = "Words", ylab = "Frequency")
matchedP <- match(names(wordCounts), posWords, nomatch = 0)
matchedP <- matchedP != 0
matchedP <- wordCounts[matchedP]
barplot(wordCounts[posWords >= 20], las = 2, cex.names = 0.75, main = "Positive Words That Showed Up 20 Times", xlab = "Words", ylab = "Frequency")
matchedP <- match(names(wordCounts), posWords, nomatch = 0)
matchedP <- matchedP != 0
matchedP <- wordCounts[matchedP]
barplot(matchedP[matchedP >= 20], las = 2, cex.names = 0.75, xlab = "Words", ylab = "Frequency")
sum(matchedN)/totalWords
matchedP <- match(names(wordCounts), posWords, nomatch = 0)
matchedP <- matchedP != 0
matchedP <- wordCounts[matchedP]
barplot(matchedP[matchedP >= 20], las = 2, cex.names = 0.75, xlab = "Words", ylab = "Frequency")
sum(matchedN)/totalWords
totalWords <- sum(wordCounts)
matchedP <- match(names(wordCounts), posWords, nomatch = 0)
matchedP <- matchedP != 0
matchedP <- wordCounts[matchedP]
barplot(matchedP[matchedP >= 20], las = 2, cex.names = 0.75, xlab = "Words", ylab = "Frequency")
sum(matchedN)/totalWords
wordCorpus <- tm_map(wordCorpus, tolower)
wordCorpus <- tm_map(wordCorpus, removePunctuation)
wordCorpus <- tm_map(wordCorpus, removeNumbers)
wordCorpus <- tm_map(wordCorpus, removeWords, stopwords("english"))
wordCorpus$content[1]
wordCorpus <- tm_map(wordCorpus, tolower)
wordCorpus <- tm_map(wordCorpus, removePunctuation)
wordCorpus <- tm_map(wordCorpus, removeNumbers)
wordCorpus <- tm_map(wordCorpus, removeWords, stopwords("english"))
wordCorpus$content[1]
#A term document matrix depicts the frequency of terms that occur in a document or multiple documents.
tdm <- TermDocumentMatrix(wordCorpus)
m <- as.matrix(tdm)
wordCounts <- rowSums(m)
wordCounts <- sort(wordCounts, decreasing = TRUE)
head(wordCounts, 10)
many_word_counts <- wordCounts[wordCounts >= 50]
barplot(many_word_counts, las = 2, cex.names = 0.75, main = "Words That Showed Up 50 Times", xlab = "Words", ylab = "Frequency")
totalWords <- sum(wordCounts)
matchedP <- match(names(wordCounts), posWords, nomatch = 0)
matchedP <- matchedP != 0
matchedP <- wordCounts[matchedP]
barplot(matchedP[matchedP >= 20], las = 2, cex.names = 0.75, xlab = "Words", ylab = "Frequency")
sum(matchedN)/totalWords
totalWords <- sum(wordCounts)
matchedP <- match(names(wordCounts), posWords, nomatch = 0)
matchedP <- matchedP != 0
matchedP <- wordCounts[matchedP]
barplot(matchedP[matchedP >= 20], las = 2, cex.names = 0.75, xlab = "Words", ylab = "Frequency")
print(sum(matchedN)/totalWords)
news <- news %>% group_by(year,month) %>% mutate(count=n(), yearmonth = paste(year, month,sep = '/')) %>% arrange(year,month,day)
print(news)
library(ggplot2)
library(dplyr)
ggplot(news, aes(x = factor(yearmonth, levels = unique(yearmonth)), fill = yearmonth)) + geom_bar() + labs(title = "Frequency of Articles", x = "Year/Month", y = "Number of Articles") + theme(axis.text=element_text(size=4,angle=90))
library(ggplot2)
library(dplyr)
ggplot(news, aes(x = factor(yearmonth, levels = unique(yearmonth)), fill = yearmonth)) + geom_bar() + labs(title = "Frequency of Articles", x = "Year/Month", y = "Number of Articles") + theme(axis.text=element_text(size=4,angle=90))
matchedN <- match(names(wordCounts), negWords, nomatch = 0)
matchedN <- matchedN != 0
matchedN <- wordCounts[matchedN]
barplot(matchedN[matchedN >= 20], las = 2, cex.names = 0.75, xlab = "Words", ylab = "Frequency")
sum(matchedN)/totalWords
news <- news %>% group_by(year,month) %>% mutate(count=n(), yearmonth = paste(year, month,sep = '/')) %>% arrange(year,month,day)
print(news)
#my version of R does not allow me to download corpus packages, therefore I cannot use term_stats() function
wordCorpus2 <- Corpus(VectorSource(news$headline_text))
wordCorpus2 <- tm_map(wordCorpus, tolower)
wordCorpus2 <- tm_map(wordCorpus, removePunctuation)
wordCorpus2 <- tm_map(wordCorpus, removeNumbers)
wordCorpus2 <- tm_map(wordCorpus, removeWords, stopwords("english"))
tdm2 <- DocumentTermMatrix(wordCorpus2)
m2 <- as.matrix(tdm2)
sortterms <- sort(colSums(m2), decreasing = TRUE)
head(sortterms, 20)
#my version of R does not allow me to download corpus packages, therefore I cannot use term_stats() function
wordCorpus2 <- Corpus(VectorSource(news$headline_text))
wordCorpus2 <- tm_map(wordCorpus, tolower)
wordCorpus2 <- tm_map(wordCorpus, removePunctuation)
wordCorpus2 <- tm_map(wordCorpus, removeNumbers)
wordCorpus2 <- tm_map(wordCorpus, removeWords, stopwords("english"))
tdm2 <- DocumentTermMatrix(wordCorpus2)
m2 <- as.matrix(tdm2)
sortterms <- sort(colSums(m2), decreasing = TRUE)
head(sortterms, 20)
dtm2 <- tokens_select(tokens, wordCorpus2, removeNumbers = TRUE, removePunctuation = TRUE)
dtm2 <- tokens_select(tokens, wordCorpus2, removeNumbers = TRUE, removePunctuation = TRUE, n = 2)
names(news) <- as.character(2:maxTopics)
names(news) <- as.character(2:maxTopics)
newscorpus <- corpus(charVector)
newscorpus_paragraphs <- corpus_reshape(newscorpus, to = "paragraphs")
news_dtm <- dfm(newscorpus_paragraphs),
install.packages("quanteda")
install.packages("corpus")
news_dtm <- dfm(newscorpus_paragraphs),
news_dtm <- dfm(newscorpus_paragraphs,
tolower = TRUE,
removePunctuation = TRUE,
removeNumbers = TRUE,
remove = stopwords("english"),
stem = TRUE)
news_dtm <- dfm(newscorpus_paragraphs,
tolower = TRUE,
removePunctuation = TRUE,
removeNumbers = TRUE,
remove = stopwords("english"),
stem = TRUE)
library("quanteda.textplots")
news_dtm <- dfm(newscorpus_paragraphs,
tolower = TRUE,
removePunctuation = TRUE,
removeNumbers = TRUE,
remove = stopwords("english"),
stem = TRUE)
newscorpus <- corpus(charVector)
library(tidyverse)
library(tm)
news<-read.csv("news.csv",header=T)
posWords <- scan("positive-words.txt", character(0), sep = "\n")  # 2006 items
negWords <- scan("negative-words.txt", character(0), sep = "\n")  # 4783 items
head(posWords,15)
head(negWords,15)
install.packages("quanteda")
install.packages("corpus")
install.packages("quanteda")
install.packages("corpus")
library("quanteda")
library('corpus')
news_dtm <- dfm(newscorpus_paragraphs,
tolower = TRUE,
removePunctuation = TRUE,
removeNumbers = TRUE,
remove = stopwords("english"),
stem = TRUE)
news_dtm <- dfm(newscorpus_paragraphs,
tolower = TRUE,
remove_symbols = TRUE
remove_punct = TRUE,
news_dtm <- dfm(newscorpus_paragraphs,
tolower = TRUE,
remove_symbols = TRUE,
remove_punct = TRUE,
removeNumbers = TRUE,
remove = stopwords("english"),
stem = TRUE)
news_dtm <- dfm(newscorpus_paragraphs,
tolower = TRUE,
remove_symbols = TRUE,
remove_punct = TRUE,
remove_numbers = TRUE,
remove = stopwords("english"),
stem = TRUE)
print(news_dtm)
print(news)
news_dtm <- dfm(newscorpus_paragraphs,
tolower = TRUE,
remove_symbols = TRUE,
remove_punct = TRUE,
remove_numbers = TRUE,
remove = stopwords("english"),
stem = TRUE)
dfm_trim(news_dtm, min_termfreq = 50)
library("quanteda.textplots")
textplot_wordcloud(news_dtm,adjust=.6)
library("topicmodels")
library('tidytext')
library("topicmodels")
library('tidytext')
topic_model <- convert(news_dtm, to = "topicmodels")
topic_model <- LDA(news_dtm, method = "VEM", k=8)
terms(topic_model,10)
tidy_topics <- tidy(news_dtm, matrix = "beta")
tidy_topics
tidy_topics <- tidy(news_dtm, matrix = "beta")
tidy_topics
anthem_top_topics <- tidy_topics %>%
group_by(topic) %>%
slice_max(beta, n = 2) %>%
ungroup() %>%
arrange(topic, -beta)
tidy_topics <- tidy(news_dtm, matrix = "beta")
tidy_topics
anthem_top_topics <- tidy_topics %>%
group_by(topic) %>%
slice_max(beta, n = 10) %>%
ungroup() %>%
arrange(topic, -beta)
tidy_topics <- tidy(topic_model, matrix = "beta")
install.packages(reshape2)
tidy_topics <- tidy(topic_model, matrix = "beta")
install.packages(reshape2)
install.packages("reshape2")
install.packages("reshape2")
tidy_topics <- tidy(topic_model, matrix = "beta")
tidy_topics
anthem_top_topics <- tidy_topics %>%
group_by(topic) %>%
slice_max(beta, n = 10) %>%
ungroup() %>%
arrange(topic, -beta)
install.packages("reshape2")
library(dplyr)
tidy_topics <- tidy(topic_model, matrix = "beta")
tidy_topics
anthem_top_topics <- tidy_topics %>%
group_by(topic) %>%
slice_max(beta, n = 10) %>%
ungroup() %>%
arrange(topic, -beta)
anthem_top_topics %>%
mutate(term = reorder_within(term, beta, topic)) %>%
ggplot(aes(beta, term, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~topic, scales = "free") + scale_y_reordered()
library(dplyr)
tidy_topics <- tidy(topic_model, matrix = "beta")
tidy_topics
anthem_top_topics <- tidy_topics %>%
group_by(topic) %>%
slice_max(beta, n = 10) %>%
ungroup() %>%
arrange(topic, -beta)
anthem_top_topics %>%
mutate(term = reorder_within(term, beta, topic)) %>%
ggplot(aes(beta, term, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~topic, scales = "free") + scale_y_reordered()
library(dplyr)
library(ggplot2)
tidy_topics <- tidy(topic_model, matrix = "beta")
tidy_topics
anthem_top_topics <- tidy_topics %>%
group_by(topic) %>%
slice_max(beta, n = 10) %>%
ungroup() %>%
arrange(topic, -beta)
anthem_top_topics %>%
mutate(term = reorder_within(term, beta, topic)) %>%
ggplot(aes(beta, term, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~topic, scales = "free") + scale_y_reordered()
tidy_anthems <- tidy(topic_model, matrix = "gamma")
tidy_anthems
tidy_anthems <- tidy(topic_model, matrix = "gamma")
tidy_anthems
tidy_anthems <- tidy_anthems %>%
top_anthems <- tidy_anthems %>%
group_by(topicname) %>%
slice_max(gamma, n = 5) %>%
ungroup() %>%
arrange(document, -gamma)
tidy_anthems <- tidy(topic_model, matrix = "gamma")
tidy_anthems
tidy_anthems <- tidy_anthems %>%
top_anthems <- tidy_anthems %>%
group_by(topic) %>%
slice_max(gamma, n = 5) %>%
ungroup() %>%
arrange(document, -gamma)
tidy_anthems <- tidy(topic_model, matrix = "gamma")
tidy_anthems
tidy_anthems <- tidy_anthems %>%
top_anthems <- tidy_anthems %>%
group_by(topic) %>%
slice_max(gamma, n = 5) %>%
ungroup() %>%
arrange(document, -gamma)
tidy_anthems <- tidy(topic_model, matrix = "gamma")
tidy_anthems
tidy_anthems <- tidy_anthems %>%
top_anthems <- tidy_anthems %>%
group_by(topic) %>%
slice_max(gamma, n = 5) %>%
ungroup() %>%
arrange(document, -gamma)
tidy_anthems <- tidy(topic_model, matrix = "gamma")
tidy_anthems
top_anthems <- tidy_anthems %>%
group_by(topic) %>%
slice_max(gamma, n = 5) %>%
ungroup() %>%
arrange(document, -gamma)
top_anthems %>%
mutate(document = reorder_within(document, gamma, topicname)) %>%
ggplot(aes(gamma, document, fill = factor(topicname))) +
geom_col(show.legend = FALSE) +
facet_wrap(~topicname, scales = "free") +
scale_y_reordered()
tidy_anthems <- tidy(topic_model, matrix = "gamma")
tidy_anthems
top_anthems <- tidy_anthems %>%
group_by(topic) %>%
slice_max(gamma, n = 5) %>%
ungroup() %>%
arrange(document, -gamma)
top_anthems %>%
mutate(document = reorder_within(document, gamma, topic)) %>%
ggplot(aes(gamma, document, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~topic, scales = "free") +
scale_y_reordered()
wordCorpus <- tm_map(wordCorpus, tolower)
wordCorpus <- tm_map(wordCorpus, tolower)
wordVector <- VectorSource(charVector)
library(tidyverse)
library(tm)
news<-read.csv("news.csv",header=T)
posWords <- scan("positive-words.txt", character(0), sep = "\n")  # 2006 items
negWords <- scan("negative-words.txt", character(0), sep = "\n")  # 4783 items
head(posWords,15)
head(negWords,15)
print(news)
#There is 18 years of data loaded in (2003-2021)
charVector <- news$headline_text
head(charVector, 6)
wordVector <- VectorSource(charVector)
wordCorpus <- Corpus(wordVector)
wordCorpus <- tm_map(wordCorpus, tolower)
wordCorpus <- tm_map(wordCorpus, removePunctuation)
wordCorpus <- tm_map(wordCorpus, removeNumbers)
wordCorpus <- tm_map(wordCorpus, removeWords, stopwords("english"))
wordCorpus$content[1]
#A term document matrix depicts the frequency of terms that occur in a document or multiple documents.
tdm <- TermDocumentMatrix(wordCorpus)
m <- as.matrix(tdm)
wordCounts <- rowSums(m)
wordCounts <- sort(wordCounts, decreasing = TRUE)
head(wordCounts, 10)
many_word_counts <- wordCounts[wordCounts >= 50]
barplot(many_word_counts, las = 2, cex.names = 0.75, main = "Words That Showed Up 50 Times", xlab = "Words", ylab = "Frequency")
totalWords <- sum(wordCounts)
matchedP <- match(names(wordCounts), posWords, nomatch = 0)
matchedP <- matchedP != 0
matchedP <- wordCounts[matchedP]
barplot(matchedP[matchedP >= 20], las = 2, cex.names = 0.75, xlab = "Words", ylab = "Frequency")
sum(matchedN)/totalWords
matchedN <- match(names(wordCounts), negWords, nomatch = 0)
matchedN <- matchedN != 0
matchedN <- wordCounts[matchedN]
barplot(matchedN[matchedN >= 20], las = 2, cex.names = 0.75, xlab = "Words", ylab = "Frequency")
sum(matchedN)/totalWords
news <- news %>% group_by(year,month) %>% mutate(count=n(), yearmonth = paste(year, month,sep = '/')) %>% arrange(year,month,day)
print(news)
library(ggplot2)
library(dplyr)
ggplot(news, aes(x = factor(yearmonth, levels = unique(yearmonth)), fill = yearmonth)) + geom_bar() + labs(title = "Frequency of Articles", x = "Year/Month", y = "Number of Articles") + theme(axis.text=element_text(size=4,angle=90))
library(ggplot2)
library(dplyr)
ggplot(news, aes(x = factor(yearmonth, levels = unique(yearmonth)))) + geom_bar() + labs(title = "Frequency of Articles", x = "Year/Month", y = "Number of Articles") + theme(axis.text=element_text(size=4,angle=90))
library(dplyr)
library(ggplot2)
tidy_topics <- tidy(topic_model, matrix = "beta")
tidy_topics
anthem_top_topics <- tidy_topics %>%
group_by(topic) %>%
slice_max(beta, n = 10) %>%
ungroup() %>%
arrange(topic, -beta)
anthem_top_topics %>%
mutate(term = reorder_within(term, beta, topic)) %>%
ggplot(aes(beta, term, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~topic, scales = "free") + scale_y_reordered()
tidy_anthems <- tidy(topic_model, matrix = "gamma")
tidy_anthems
top_anthems <- tidy_anthems %>%
group_by(topic) %>%
slice_max(gamma, n = 5) %>%
ungroup() %>%
arrange(document, -gamma)
top_anthems %>%
mutate(document = reorder_within(document, gamma, topic)) %>%
ggplot(aes(gamma, document, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~topic, scales = "free") +
scale_y_reordered()
tidy_anthems <- tidy(topic_model, matrix = "gamma")
tidy_anthems
top_anthems <- tidy_anthems %>%
group_by(topic) %>%
slice_max(gamma, n = 5) %>%
ungroup() %>%
arrange(document, -gamma)
top_anthems %>%
mutate(document = reorder_within(document, gamma, topic)) %>%
ggplot(aes(gamma, document, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~topic, scales = "free") +
scale_y_reordered()
